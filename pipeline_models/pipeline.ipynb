{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recognized-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "############\n",
    "# INSTALLS #\n",
    "############\n",
    "\n",
    "#Abstractive Summarizer Installs\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install rouge_score\n",
    "!pip install sacrebleu\n",
    "\n",
    "#Extractive Summarizer Installs\n",
    "!pip install bert-extractive-summarizer\n",
    "!pip install neuralcoref\n",
    "!pip install spacy==2.1.3\n",
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "threaded-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "\n",
    "#Abstractive Summarizer Imports\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "\n",
    "#Extractive Summarizer Imports\n",
    "from tqdm import tqdm_pandas\n",
    "from summarizer import Summarizer\n",
    "\n",
    "#Cosine Similarity Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "#Utility Imports\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Dataset Library Imports\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "promising-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "###############\n",
    "# GLOBAL VARS #\n",
    "###############\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")  \n",
    "abstractive_summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
    "extractive_summarizer_model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "precious-tolerance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/home/jupyter/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602)\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# DATA #\n",
    "########\n",
    "\n",
    "#CNN Dailymail dataset, initially used for testing summarizers\n",
    "test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "twenty_news_dataset = fetch_20newsgroups()\n",
    "\n",
    "#Toy data for testing the pipeline\n",
    "covid_test_data = pd.read_csv(\"/home/jupyter/266/266_final/nyt_data_collection/toy_data/fp_covid_articles.csv\")\n",
    "golf_test_data = pd.read_csv(\"/home/jupyter/266/266_final/nyt_data_collection/toy_data/golf_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "thorough-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "# MISC Helpers\n",
    "divString = lambda size, char = \"#\": reduce(add, [char for i in range(size)])\n",
    "flatten = lambda lst: [i for sublst in lst for i in sublst]\n",
    "\n",
    "\n",
    "#Batch Summary Generation and Batch Metrics\n",
    "def generate_summary(batch):\n",
    "    \"\"\"This function computes a summary for a given article from the Dataset object\n",
    "    batch\n",
    "    Params:\n",
    "    batch: an article from the given Dataset object.\"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    batch[\"pred\"] = output_str\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compute_metrics(batch, batch_size=16, metric_name=\"rouge\"):\n",
    "    \"\"\"This function computes the rouge or bleu scores for predicted summaries\n",
    "    Params:\n",
    "    batch: A Dataset object which contains the articles at the specified indices\n",
    "    Use the select method for this function call. \n",
    "    Example format: Dataset.select([list of indices to select from the original dataset])\n",
    "    metric_name: The prefered evaluation metric to use\"\"\"\n",
    "    \n",
    "    metric = datasets.load_metric(metric_name)\n",
    "    results = batch.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "    summary_pred = results[\"pred\"]\n",
    "    label_ref = results[\"highlights\"]\n",
    "    if metric_name == \"rouge\":\n",
    "        output = metric.compute(predictions=summary_pred, references=label_ref, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        print(\"\\n\" + \"ROUGE SCORE:\")\n",
    "        return output\n",
    "    else:\n",
    "        # Else compute bleu score with metric name \"sacrebleu\"\n",
    "        all_bleu_scores = []\n",
    "        for i in range(len(batch)):\n",
    "            output = metric.compute(predictions= [summary_pred[i]], references= [[label_ref[i]]])\n",
    "            all_bleu_scores.append(output)\n",
    "            print(\"\\n\\n\")\n",
    "            print(divString(100))\n",
    "            print(\"\\n\\n\" + \"Summary prediction: \" + \"\\n\\n\", summary_pred[i])\n",
    "            print(\"\\n\\n\" + \"Reference Label: \" + \"\\n\\n\", label_ref[i])\n",
    "            print(\"\\n\\n\" + \"BLEU SCORE:\" + \"\\n\\n\", output)\n",
    "            print(\"\\n\")\n",
    "        return all_bleu_scores\n",
    "    \n",
    "\n",
    "#Raw Text Summarization\n",
    "def generate_abstractive_summary(raw_string, model = abstractive_summarizer_model):\n",
    "    \"\"\"This function produces an abstractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An abstractive summarizer model\"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(raw_string, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_str[0]\n",
    "\n",
    "\n",
    "def generate_extractive_summary(raw_string, model = extractive_summarizer_model, min_summary_length = 50):\n",
    "    \"\"\"This function produces an extractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An extractive summarizer model\"\"\"\n",
    "    output_str = model(raw_string, min_length = min_summary_length)\n",
    "    return output_str\n",
    "    \n",
    "    \n",
    "#Search and Subset Dataset\n",
    "def search_and_subset_data(df, keyword, column = \"first_paragraph\"):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, keyword to search, and an optional column to search through and returns a\n",
    "    subset of the data as a pandas dataframe, with entries that contain the searched keyword.\n",
    "    Params:\n",
    "    df: Dataframe\n",
    "    keyword: A keyword to search\n",
    "    column: Optional that takes either 'first_paragraph' or 'keywords'\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by='date', ascending = False).reset_index().drop(\"index\", axis = 1) # Ordering all documents chronologically so that indices don't need reodered when combining similar documents\n",
    "    df = df.dropna(subset=[column])\n",
    "    subset = df[df[column].str.lower().str.contains(keyword.strip().lower())]\n",
    "    return subset\n",
    "\n",
    "\n",
    "def select_random_document(df):\n",
    "    \"\"\"\n",
    "    This function selects a single random row from a dataframe. This is used as a default for initial document selection at the beginning of the pipeline.\n",
    "    df: A pandas dataframe\n",
    "    \"\"\"\n",
    "    return df.sample()\n",
    "\n",
    "\n",
    "\n",
    "# Similarity Clustering and Aggregate Document Synthesis\n",
    "def compute_cosine_similarities(document, corpus, vectorizer = vectorizer):\n",
    "    \"\"\"\n",
    "    This function computes the cosine similarity between a document and a specified corpus of documents.\n",
    "    document: A string of text.\n",
    "    corpus: An array of documents.\n",
    "    vectorizer: A TfidfVectorizer() object. (Default: initialized in the constants cell)\n",
    "    \"\"\"\n",
    "    vectorized_corpus = vectorizer.fit_transform(corpus)\n",
    "    vectorized_document = vectorizer.transform([document])\n",
    "    return linear_kernel(vectorized_document, vectorized_corpus).flatten()\n",
    "\n",
    "\n",
    "def get_related_docs_indices(cos_similarities_array, n_docs=5):\n",
    "    \"\"\"\n",
    "    This function returns the document indices with the highest cosine similarity.\n",
    "    cos_similarities_array: An array of the computed cosine similarities for the whole corpus.\n",
    "    n_docs: The number of highest scoring documents to return. (e.g. n_docs=5 returns the top 5 highest scoring documents)\n",
    "    \"\"\"\n",
    "    cos_similarities_array = np.array([i for i in cos_similarities_array if i < 0.999999999]) # This eliminates the case where the most similar document is the document itself, which has a similarity of 1.0\n",
    "    return sorted(cos_similarities_array.argsort()[:-(n_docs + 1):-1])\n",
    "    \n",
    "\n",
    "def get_top_similarities(cos_similarities_array, n_docs=5):\n",
    "    \"\"\"\n",
    "    This function returns the highest document cosine similarity scores.\n",
    "    cos_similarities_array: An array of the computed cosine similarities for the whole corpus.\n",
    "    n_docs: The number of highest cosine scores to return. (e.g. n_docs=5 returns the top 5 highest cosine similarity scores)\n",
    "    \"\"\"\n",
    "    related_indices = get_related_docs_indices(cos_similarities_array, n_docs)\n",
    "    return cos_similarities_array[related_indices]\n",
    "\n",
    "\n",
    "def concatenate_related_docs(corpus, related_docs_indices):\n",
    "    docs = [corpus[i] for i in related_docs_indices]\n",
    "    return \" \".join(docs)\n",
    "    \n",
    "\n",
    "# Display Functions\n",
    "def show_related_docs(document, corpus, related_docs_indices):\n",
    "    \"\"\"\n",
    "    This function displays the seed document, selected similar documents, and the concatenated aggregate of the similar documents.\n",
    "    \"\"\"\n",
    "    aggregate_doc = concatenate_related_docs(corpus, related_docs_indices)\n",
    "    print(\"\\n\" + \"SELECTED DOCUMENT: \" + \"\\n\")\n",
    "    print(document)\n",
    "    print(\"\\n\")\n",
    "    print(divString(100))\n",
    "    print(\"\\n\")\n",
    "    print(\"SIMILAR DOCUMENTS: \" + \"\\n\")\n",
    "    for i in related_docs_indices:\n",
    "        print(corpus[i], \"\\n\")\n",
    "        print(divString(100, char = \"~\") + \"\\n\")\n",
    "    print(divString(100))\n",
    "    print(\"\\n\")\n",
    "    print(\"AGGREGATE DOCUMENT: \" + \"\\n\")\n",
    "    print(aggregate_doc + \"\\n\")\n",
    "    print(divString(100))\n",
    "\n",
    "\n",
    "#Pipeline Iteration\n",
    "def iterate_pipeline(initial_document, corpus, num_iter = 5, aggregate_doc_variant = \"abstractive_secondary\"):\n",
    "    \"\"\"\n",
    "    This function begins with an initial document. From there the initial document is clustered with similar documents, \n",
    "    \"\"\"\n",
    "    document = initial_document\n",
    "    for i in range(num_iter):\n",
    "        cosine_similarities = compute_cosine_similarities(document, corpus)\n",
    "        related_docs_indices = get_related_docs_indices(cosine_similarities)\n",
    "        aggregate_document = concatenate_related_docs(corpus, related_docs_indices)\n",
    "        if aggregate_doc_variant == \"abstractive_primary\":\n",
    "            document = generate_abstractive_summary(aggregate_document, model = abstractive_summarizer_model)\n",
    "        if aggregate_doc_variant == \"abstractive_secondary\":\n",
    "            extractive_summary_primary = generate_extractive_summary(aggregate_document, min_summary_length=100)\n",
    "            document = generate_abstractive_summary(extractive_summary_primary, model = abstractive_summarizer_model)\n",
    "        if aggregate_doc_variant == \"extractive_primary\":\n",
    "            document = generate_extractive_summary(aggregate_document, min_summary_length=100)\n",
    "    print(\"Seed Document:\" + \"\\n\")\n",
    "    print(initial_document, \"\\n\")\n",
    "    print(divString(100) + \"\\n\")\n",
    "    print(\"Resulting Document:\" + \"\\n\")\n",
    "    print(document, \"\\n\")\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "considerable-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function Unit Tests\n",
    "\n",
    "# generate_summary(test_data[0])\n",
    "# compute_metrics(test_data.select([1,2]), metric_name = \"rouge\")\n",
    "# compute_metrics(test_data.select([1, 2]), metric_name = \"sacrebleu\")\n",
    "# generate_extractive_summary(test_data[0][\"article\"])\n",
    "# generate_extractive_summary(test_data[0][\"article\"], min_summary_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-profession",
   "metadata": {},
   "source": [
    "### Baseline Algorithm Walkthrough\n",
    "1. Select a group/subset of articles (Order the subset chronologically so that the dataframe indices are chronological)\n",
    "2. Select a single article from the subset produced in step 1. (Baseline selection will be random)\n",
    "3. Perform cosine similarity between the single/selected article and the entire subset of articles produced in step 1.\n",
    "4. Select the top most similar indices and their associated articles.\n",
    "5. Summarize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "rubber-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# PIPELINE #\n",
    "############\n",
    "\n",
    "# Document Selection\n",
    "search_results_df = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "corpus = search_results_df.first_paragraph.to_list()  # Converting to a list to vectorize the entries\n",
    "document = select_random_document(search_results_df).first_paragraph.values[0] # Selected a random row from the search results dataframe and extracted the first paragraph text to serve as our document\n",
    "\n",
    "# Similarity Clustering and Aggregate Document Synthesis\n",
    "cosine_similarities = compute_cosine_similarities(document, corpus) # The cosine similarities between the target document and all documents contained in the corpus\n",
    "related_docs_indices = get_related_docs_indices(cosine_similarities) # The indices of the most related docs\n",
    "aggregate_document = concatenate_related_docs(corpus, related_docs_indices) # The resulting document that is produced by concatenating all of the most similar documents\n",
    "\n",
    "# Summarization\n",
    "extractive_summary_primary = generate_extractive_summary(aggregate_document, min_summary_length=100)\n",
    "abstractive_summary_primary = generate_abstractive_summary(aggregate_document, model = abstractive_summarizer_model)\n",
    "abstractive_summary_secondary = generate_abstractive_summary(extractive_summary_primary, model = abstractive_summarizer_model) #Secondary summary in the hierarchy (i.e. a summary of a summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "generous-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Cosine Similarity Scores:  [0.14345526 0.14457817 0.04430281 0.13621813 0.08780106]\n",
      "\n",
      "SELECTED DOCUMENT: \n",
      "\n",
      "The death of Herman Cain, attributed to the coronavirus, has made Republicans and President Trump face the reality of the pandemic as it hit closer to home than ever before, claiming a prominent conservative ally whose frequently dismissive attitude about taking the threat seriously reflected the hands-off inconsistency of party leaders.\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "SIMILAR DOCUMENTS: \n",
      "\n",
      "WASHINGTON — President Trump on Wednesday rejected the professional scientific conclusions of his own government about the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus as the death toll in the United States from the disease neared 200,000. \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "President Trump on Wednesday rejected the professional scientific conclusions of his own government about the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus as the death toll in the United States from the disease neared 200,000. \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "A school principal, a nurse, an actor. A Tony-winning playwright, a pediatric neurosurgeon, a jazz pianist. The coronavirus epidemic is an undiscriminating reaper. It has brought death to all walks of life, all economic backgrounds, all races and all the continents. \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "WASHINGTON — The numbers the health officials showed President Trump were overwhelming. With the peak of the coronavirus pandemic still weeks away, he was told, hundreds of thousands of Americans could face death if the country reopened too soon. \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "With the death toll from the coronavirus surpassing 3,000 worldwide, the ramifications have spread to nearly every aspect of life, with sports no exception. Events major and minor have been canceled, moved or postponed, as athletes, officials and spectators worry about the spread of the virus. \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "AGGREGATE DOCUMENT: \n",
      "\n",
      "WASHINGTON — President Trump on Wednesday rejected the professional scientific conclusions of his own government about the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus as the death toll in the United States from the disease neared 200,000. President Trump on Wednesday rejected the professional scientific conclusions of his own government about the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus as the death toll in the United States from the disease neared 200,000. A school principal, a nurse, an actor. A Tony-winning playwright, a pediatric neurosurgeon, a jazz pianist. The coronavirus epidemic is an undiscriminating reaper. It has brought death to all walks of life, all economic backgrounds, all races and all the continents. WASHINGTON — The numbers the health officials showed President Trump were overwhelming. With the peak of the coronavirus pandemic still weeks away, he was told, hundreds of thousands of Americans could face death if the country reopened too soon. With the death toll from the coronavirus surpassing 3,000 worldwide, the ramifications have spread to nearly every aspect of life, with sports no exception. Events major and minor have been canceled, moved or postponed, as athletes, officials and spectators worry about the spread of the virus.\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "SUMMARIZATION RESULTS\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "EXTRACTIVE SUMMARY PRIMARY:\n",
      " WASHINGTON — President Trump on Wednesday rejected the professional scientific conclusions of his own government about the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus as the death toll in the United States from the disease neared 200,000. President Trump on Wednesday rejected the professional scientific conclusions of his own government about the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus as the death toll in the United States from the disease neared 200,000.\n",
      "\n",
      "\n",
      "ABSTRACTIVE SUMMARY PRIMARY:\n",
      " president trump rejected the professional scientific conclusions of his own government about the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus as the death toll from the disease reached 200, 000. a school principal, a nurse, an actor and an actor were among those who could face death by lethal injection.\n",
      "\n",
      "\n",
      "ABSTRACTIVE SUMMARY SECONDARY:\n",
      " president trump rejected the professional scientific conclusions of his own government. the death toll in the u. s. from the disease reached 200, 000. president trump on wednesday rejected the pros'conclusions of the prospects for a widely available coronavirus vaccine and the effectiveness of masks in curbing the spread of the virus.\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Cosine Similarity Scores: \", get_top_similarities(cosine_similarities))\n",
    "show_related_docs(document, corpus, related_docs_indices)\n",
    "\n",
    "print(\"\\n\" + \"SUMMARIZATION RESULTS\" + \"\\n\")\n",
    "print(divString(100) + \"\\n\")\n",
    "print(\"EXTRACTIVE SUMMARY PRIMARY:\" + \"\\n\", extractive_summary_primary)\n",
    "print(\"\\n\")\n",
    "print(\"ABSTRACTIVE SUMMARY PRIMARY:\" + \"\\n\", abstractive_summary_primary)\n",
    "print(\"\\n\")\n",
    "print(\"ABSTRACTIVE SUMMARY SECONDARY:\" + \"\\n\", abstractive_summary_secondary)\n",
    "print(divString(100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "abstract-diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Document:\n",
      "\n",
      "A surge in coronavirus deaths in the United States has prompted the vast majority of governors to order their residents to stay home, but a small number of states are resisting increasingly urgent calls to shut down. \n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "Resulting Document:\n",
      "\n",
      "the centers for disease control and prevention has been warning that a more contagious variant of the coronavirus may become dominant in the united states by march. a more likely variant of this coronavirus is likely to become the dominant source of infection in the u. s. by march, perhaps leading to a wrenching surge in cases and deaths. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the centers for disease control and prevention has been warning that a more contagious variant of the coronavirus may become dominant in the united states by march. a more likely variant of this coronavirus is likely to become the dominant source of infection in the u. s. by march, perhaps leading to a wrenching surge in cases and deaths.'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################\n",
    "# ITERATIVE PIPELINE #\n",
    "######################\n",
    "\n",
    "# Initial Document Selection\n",
    "search_results_df = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "corpus = search_results_df.first_paragraph.to_list()  # Converting to a list to vectorize the entries\n",
    "initial_document = select_random_document(search_results_df).first_paragraph.values[0] # Selected a random row from the search results dataframe and extracted the first paragraph text to serve as our document\n",
    "\n",
    "# Pipeline Iteration\n",
    "iterate_pipeline(initial_document, corpus, num_iter = 5, aggregate_doc_variant = \"abstractive_secondary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-recorder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "balanced-civilian",
   "metadata": {},
   "source": [
    "# Scratch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acquired-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_df = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "search_results = search_results_df.first_paragraph.to_list()  # Converting to a list to vectorize the entries\n",
    "corpus = vectorizer.fit_transform(search_results) # Vectorize and fit transform\n",
    "document = select_random_document(x).first_paragraph.values[0] # Selected a random row from the search results dataframe and extracted the first paragraph text to serve as our document\n",
    "# compute_cosine_similarities(document, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "collected-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WASHINGTON — The Trump administration, racing a surging Covid-19 death toll, instructed states on Tuesday to immediately begin vaccinating every American 65 and older, as well as tens of millions of adults with medical conditions that put them at higher risk of dying from coronavirus infection.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_random_document(x).first_paragraph.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "arbitrary-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.07616775, 0.02808496, 0.11774414, 0.03636491,\n",
       "       0.11259535, 0.10315487, 0.12655489, 0.05390195, 0.08478859,\n",
       "       0.08520882, 0.07082236, 0.07575236, 0.04181228, 0.03807006,\n",
       "       0.10346441, 0.13086099, 0.05104183, 0.08180416, 0.09875876,\n",
       "       0.06415114, 0.0425284 , 0.0628859 , 0.13876087, 0.0863506 ,\n",
       "       0.06123893, 0.05645384, 0.11517398, 0.02818955, 0.03787033,\n",
       "       0.08467625, 0.0716679 , 0.07715621, 0.17073456, 0.06300363,\n",
       "       0.10544956, 0.03742148, 0.05863446, 0.11771831, 0.07676028,\n",
       "       0.1086421 , 0.09694976, 0.04841343, 0.05976972, 0.09376444,\n",
       "       0.03410992, 0.10781565, 0.10350352, 0.08012644, 0.0365176 ,\n",
       "       0.15337929, 0.08447607, 0.06627055, 0.03930884, 0.04738421,\n",
       "       0.06617171, 0.04870063, 0.06727244, 0.0839498 , 0.02872655,\n",
       "       0.09031904, 0.03866   , 0.12150809, 0.15830899, 0.02474477,\n",
       "       0.11012431, 0.09457802, 0.05708611, 0.0213857 , 0.11309312,\n",
       "       0.02541409, 0.06927503, 0.06761577, 0.03243748, 0.08644175,\n",
       "       0.06731182, 0.0195447 , 0.08269968, 0.03564938, 0.08494442,\n",
       "       0.08494442, 0.02148494, 0.09046169, 0.0335625 , 0.06097859,\n",
       "       0.06097859, 0.06097859, 0.04113102, 0.08993779, 0.06201123,\n",
       "       0.15142703, 0.03549765, 0.17523204, 0.08759292, 0.05399896,\n",
       "       0.06308317, 0.03600274, 0.09213782, 0.11896238, 0.08188011,\n",
       "       0.06306207, 0.02062626, 0.10531524, 0.09005493, 0.09075985,\n",
       "       0.07879572, 0.10164347, 0.11982562, 0.05036334, 0.04029089,\n",
       "       0.15075563, 0.02397017, 0.3283991 , 0.06816539, 0.1072576 ,\n",
       "       0.06516116, 0.11551457, 0.0706843 , 0.06760437, 0.02787715,\n",
       "       0.07879069, 0.11901036, 0.06975917, 0.05022618, 0.01516106,\n",
       "       0.01516106, 0.01516106, 0.07688647, 0.01516106, 0.05611175,\n",
       "       0.01516106, 0.01516106, 0.01516106, 0.16709947, 0.04769215,\n",
       "       0.01516106, 0.29400689, 0.01516106, 0.07052494, 0.07288961,\n",
       "       0.12092951, 0.06466674, 0.06078831, 0.09869967, 0.07530562,\n",
       "       0.06797611, 0.16047881, 0.11784632, 0.04283617, 0.05299051,\n",
       "       0.04594055, 0.06133041, 0.01894848, 0.06663872, 0.01525785,\n",
       "       0.01894848, 0.06448144, 0.03730793, 0.01894848, 0.0317711 ,\n",
       "       0.01525785, 0.01319613, 0.01525785, 0.01525785, 0.09190207,\n",
       "       0.07772648, 0.01525785, 0.01894848, 0.0406913 , 0.00860302,\n",
       "       0.07302851, 0.08015873, 0.01894848, 0.10786538, 0.1133727 ,\n",
       "       0.07824737, 0.06959981, 0.04665942, 0.03411702, 0.04911315,\n",
       "       0.04638308, 0.08402049, 0.06163985, 0.1415786 , 0.07017594,\n",
       "       0.1373735 , 0.14173402, 0.06255293, 0.0463418 , 0.09491256,\n",
       "       0.15478556, 0.09939211, 0.03376037, 0.0220329 , 0.0423412 ,\n",
       "       0.02262538, 0.08864369, 0.05701875, 0.03473974, 0.04322259,\n",
       "       0.10209256, 0.12128159, 0.09953685, 0.10856294, 0.0458036 ,\n",
       "       0.05871116, 0.08734848, 0.05600513, 0.07087905, 0.04095533,\n",
       "       0.03800507, 0.14589987, 0.10590478, 0.03833537, 0.06842203,\n",
       "       0.05167068, 0.08888843, 0.09859514, 0.0524235 , 0.0580726 ,\n",
       "       0.01894848, 0.05007482, 0.07286105, 0.08460698, 0.03666912,\n",
       "       0.03787766, 0.17027416, 0.10886585, 0.05313678, 0.06022244,\n",
       "       0.03797255, 0.15546299, 0.10657153, 0.0491613 , 0.07226347,\n",
       "       0.13287194, 0.15169117, 0.06805258, 0.09781865, 0.03135607,\n",
       "       0.06636137, 0.05408887, 0.09830824, 0.1276903 , 0.05341833,\n",
       "       0.04833081, 0.06549197, 0.05932607, 0.08388338, 0.05815308,\n",
       "       0.01894848, 0.05668227, 0.07842358, 0.14545502, 0.05360415,\n",
       "       0.11534298, 0.07071155, 0.13936938, 0.07322193, 0.07226164,\n",
       "       0.15501045, 0.1640008 , 0.05911027])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "y = x.first_paragraph.to_list() # Converting to a list to vectorize the entries\n",
    "z = vectorizer.fit_transform(y) # Vectorize and fit transform\n",
    "\n",
    "cosine_similarities = linear_kernel(z[0], z).flatten() #Compute the similarities between a single vectorized article and the rest of the vectorized corpus\n",
    "cosine_similarities"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
