{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "import tensorflow as tf\n",
    "\n",
    "#Abstractive Summarizer Imports\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "\n",
    "#Extractive Summarizer Imports\n",
    "from tqdm import tqdm_pandas\n",
    "from summarizer import Summarizer\n",
    "\n",
    "#Cosine Similarity Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "#Utility Imports\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Dataset Library Imports\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "###############\n",
    "# GLOBAL VARS #\n",
    "###############\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")  \n",
    "abstractive_summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
    "extractive_summarizer_model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# DATA #\n",
    "########\n",
    "\n",
    "#CNN Dailymail dataset, initially used for testing summarizers\n",
    "test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "twenty_news_dataset = fetch_20newsgroups()\n",
    "\n",
    "#Toy data for testing the pipeline\n",
    "covid_test_data = pd.read_csv(\"/home/jupyter/266/266_final/nyt_data_collection/toy_data/fp_covid_articles.csv\")\n",
    "golf_test_data = pd.read_csv(\"/home/jupyter/266/266_final/nyt_data_collection/toy_data/golf_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "# MISC Helpers\n",
    "divString = lambda size, char = \"#\": reduce(add, [char for i in range(size)])\n",
    "flatten = lambda lst: [i for sublst in lst for i in sublst]\n",
    "\n",
    "\n",
    "#Batch Summary Generation and Batch Metrics\n",
    "def generate_summary(batch):\n",
    "    \"\"\"This function computes a summary for a given article from the Dataset object\n",
    "    batch\n",
    "    Params:\n",
    "    batch: an article from the given Dataset object.\"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    batch[\"pred\"] = output_str\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compute_metrics(batch, batch_size=16, metric_name=\"rouge\"):\n",
    "    \"\"\"This function computes the rouge or bleu scores for predicted summaries\n",
    "    Params:\n",
    "    batch: A Dataset object which contains the articles at the specified indices\n",
    "    Use the select method for this function call. \n",
    "    Example format: Dataset.select([list of indices to select from the original dataset])\n",
    "    metric_name: The prefered evaluation metric to use\"\"\"\n",
    "    \n",
    "    metric = datasets.load_metric(metric_name)\n",
    "    results = batch.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "    summary_pred = results[\"pred\"]\n",
    "    label_ref = results[\"highlights\"]\n",
    "    if metric_name == \"rouge\":\n",
    "        output = metric.compute(predictions=summary_pred, references=label_ref, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        print(\"\\n\" + \"ROUGE SCORE:\")\n",
    "        return output\n",
    "    else:\n",
    "        # Else compute bleu score with metric name \"sacrebleu\"\n",
    "        all_bleu_scores = []\n",
    "        for i in range(len(batch)):\n",
    "            output = metric.compute(predictions= [summary_pred[i]], references= [[label_ref[i]]])\n",
    "            all_bleu_scores.append(output)\n",
    "            print(\"\\n\\n\")\n",
    "            print(divString(100))\n",
    "            print(\"\\n\\n\" + \"Summary prediction: \" + \"\\n\\n\", summary_pred[i])\n",
    "            print(\"\\n\\n\" + \"Reference Label: \" + \"\\n\\n\", label_ref[i])\n",
    "            print(\"\\n\\n\" + \"BLEU SCORE:\" + \"\\n\\n\", output)\n",
    "            print(\"\\n\")\n",
    "        return all_bleu_scores\n",
    "    \n",
    "\n",
    "#Raw Text Summarization\n",
    "def generate_abstractive_summary(raw_string, model = abstractive_summarizer_model):\n",
    "    \"\"\"This function produces an abstractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An abstractive summarizer model\"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(raw_string, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_str[0]\n",
    "\n",
    "\n",
    "def generate_extractive_summary(raw_string, model = extractive_summarizer_model, min_summary_length = 50):\n",
    "    \"\"\"This function produces an extractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An extractive summarizer model\"\"\"\n",
    "    output_str = model(raw_string, min_length = min_summary_length)\n",
    "    return output_str\n",
    "    \n",
    "    \n",
    "#Search and Subset Dataset\n",
    "def search_and_subset_data(df, keyword, column = \"first_paragraph\"):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, keyword to search, and an optional column to search through and returns a\n",
    "    subset of the data as a pandas dataframe, with entries that contain the searched keyword.\n",
    "    Params:\n",
    "    df: Dataframe\n",
    "    keyword: A keyword to search\n",
    "    column: Optional that takes either 'first_paragraph' or 'keywords'\n",
    "    \"\"\"\n",
    "    df = df.sort_values(by='date', ascending = False).reset_index().drop(\"index\", axis = 1) # Ordering all documents chronologically so that indices don't need reodered when combining similar documents\n",
    "    df = df.dropna(subset=[column])\n",
    "    subset = df[df[column].str.lower().str.contains(keyword.strip().lower())]\n",
    "    return subset\n",
    "\n",
    "\n",
    "def select_random_document(df):\n",
    "    \"\"\"\n",
    "    This function selects a single random row from a dataframe. This is used as a default for initial document selection at the beginning of the pipeline.\n",
    "    df: A pandas dataframe\n",
    "    \"\"\"\n",
    "    return df.sample()\n",
    "\n",
    "\n",
    "\n",
    "# Similarity Clustering and Aggregate Document Synthesis\n",
    "def compute_cosine_similarities(document, corpus, vectorizer = vectorizer):\n",
    "    \"\"\"\n",
    "    This function computes the cosine similarity between a document and a specified corpus of documents.\n",
    "    document: A string of text.\n",
    "    corpus: An array of documents.\n",
    "    vectorizer: A TfidfVectorizer() object. (Default: initialized in the constants cell)\n",
    "    \"\"\"\n",
    "    vectorized_corpus = vectorizer.fit_transform(corpus)\n",
    "    vectorized_document = vectorizer.transform([document])\n",
    "    return linear_kernel(vectorized_document, vectorized_corpus).flatten()\n",
    "\n",
    "\n",
    "def get_related_docs_indices(cos_similarities_array, n_docs=5):\n",
    "    \"\"\"\n",
    "    This function returns the document indices with the highest cosine similarity.\n",
    "    cos_similarities_array: An array of the computed cosine similarities for the whole corpus.\n",
    "    n_docs: The number of highest scoring documents to return. (e.g. n_docs=5 returns the top 5 highest scoring documents)\n",
    "    \"\"\"\n",
    "    cos_similarities_array = np.array([i for i in cos_similarities_array if i < 0.999999999]) # This eliminates the case where the most similar document is the document itself, which has a similarity of 1.0\n",
    "    return sorted(cos_similarities_array.argsort()[:-(n_docs + 1):-1])\n",
    "    \n",
    "\n",
    "def get_top_similarities(cos_similarities_array, n_docs=5):\n",
    "    \"\"\"\n",
    "    This function returns the highest document cosine similarity scores.\n",
    "    cos_similarities_array: An array of the computed cosine similarities for the whole corpus.\n",
    "    n_docs: The number of highest cosine scores to return. (e.g. n_docs=5 returns the top 5 highest cosine similarity scores)\n",
    "    \"\"\"\n",
    "    related_indices = get_related_docs_indices(cos_similarities_array, n_docs)\n",
    "    return cos_similarities_array[related_indices]\n",
    "\n",
    "\n",
    "def concatenate_related_docs(corpus, related_docs_indices):\n",
    "    docs = [corpus[i] for i in related_docs_indices]\n",
    "    return \" \".join(docs)\n",
    "    \n",
    "\n",
    "# Display Functions\n",
    "def show_related_docs(document, corpus, related_docs_indices):\n",
    "    \"\"\"\n",
    "    This function displays the seed document, selected similar documents, and the concatenated aggregate of the similar documents.\n",
    "    \"\"\"\n",
    "    aggregate_doc = concatenate_related_docs(corpus, related_docs_indices)\n",
    "    print(\"\\n\" + \"SELECTED DOCUMENT: \" + \"\\n\")\n",
    "    print(document)\n",
    "    print(\"\\n\")\n",
    "    print(divString(100))\n",
    "    print(\"\\n\")\n",
    "    print(\"SIMILAR DOCUMENTS: \" + \"\\n\")\n",
    "    for i in related_docs_indices:\n",
    "        print(corpus[i], \"\\n\")\n",
    "        print(divString(100, char = \"~\") + \"\\n\")\n",
    "    print(divString(100))\n",
    "    print(\"\\n\")\n",
    "    print(\"AGGREGATE DOCUMENT: \" + \"\\n\")\n",
    "    print(aggregate_doc + \"\\n\")\n",
    "    print(divString(100))\n",
    "\n",
    "\n",
    "#Pipeline Iteration\n",
    "def iterate_pipeline(initial_document, corpus, num_iter = 5, aggregate_doc_variant = \"abstractive_secondary\"):\n",
    "    \"\"\"\n",
    "    This function begins with an initial document. From there the initial document is clustered with similar documents, \n",
    "    \"\"\"\n",
    "    document = initial_document\n",
    "    for i in range(num_iter):\n",
    "        cosine_similarities = compute_cosine_similarities(document, corpus)\n",
    "        related_docs_indices = get_related_docs_indices(cosine_similarities)\n",
    "        aggregate_document = concatenate_related_docs(corpus, related_docs_indices)\n",
    "        if aggregate_doc_variant == \"abstractive_primary\":\n",
    "            document = generate_abstractive_summary(aggregate_document, model = abstractive_summarizer_model)\n",
    "        if aggregate_doc_variant == \"abstractive_secondary\":\n",
    "            extractive_summary_primary = generate_extractive_summary(aggregate_document, min_summary_length=100)\n",
    "            document = generate_abstractive_summary(extractive_summary_primary, model = abstractive_summarizer_model)\n",
    "        if aggregate_doc_variant == \"extractive_primary\":\n",
    "            document = generate_extractive_summary(aggregate_document, min_summary_length=100)\n",
    "    print(\"Seed Document:\" + \"\\n\")\n",
    "    print(initial_document, \"\\n\")\n",
    "    print(divString(100) + \"\\n\")\n",
    "    print(\"Resulting Document:\" + \"\\n\")\n",
    "    print(document, \"\\n\")\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function Unit Tests\n",
    "\n",
    "# generate_summary(test_data[0])\n",
    "# compute_metrics(test_data.select([1,2]), metric_name = \"rouge\")\n",
    "# compute_metrics(test_data.select([1, 2]), metric_name = \"sacrebleu\")\n",
    "# generate_extractive_summary(test_data[0][\"article\"])\n",
    "# generate_extractive_summary(test_data[0][\"article\"], min_summary_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Algorithm Walkthrough\n",
    "1. Select a group/subset of articles (Order the subset chronologically so that the dataframe indices are chronological)\n",
    "2. Select a single article from the subset produced in step 1. (Baseline selection will be random)\n",
    "3. Perform cosine similarity between the single/selected article and the entire subset of articles produced in step 1.\n",
    "4. Select the top most similar indices and their associated articles.\n",
    "5. Summarize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# PIPELINE #\n",
    "############\n",
    "\n",
    "# Document Selection\n",
    "search_results_df = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "corpus = search_results_df.first_paragraph.to_list()  # Converting to a list to vectorize the entries\n",
    "document = select_random_document(search_results_df).first_paragraph.values[0] # Selected a random row from the search results dataframe and extracted the first paragraph text to serve as our document\n",
    "\n",
    "# Similarity Clustering and Aggregate Document Synthesis\n",
    "cosine_similarities = compute_cosine_similarities(document, corpus) # The cosine similarities between the target document and all documents contained in the corpus\n",
    "related_docs_indices = get_related_docs_indices(cosine_similarities) # The indices of the most related docs\n",
    "aggregate_document = concatenate_related_docs(corpus, related_docs_indices) # The resulting document that is produced by concatenating all of the most similar documents\n",
    "\n",
    "# Summarization\n",
    "extractive_summary_primary = generate_extractive_summary(aggregate_document, min_summary_length=100)\n",
    "abstractive_summary_primary = generate_abstractive_summary(aggregate_document, model = abstractive_summarizer_model)\n",
    "abstractive_summary_secondary = generate_abstractive_summary(extractive_summary_primary, model = abstractive_summarizer_model) #Secondary summary in the hierarchy (i.e. a summary of a summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Cosine Similarity Scores: \", get_top_similarities(cosine_similarities))\n",
    "show_related_docs(document, corpus, related_docs_indices)\n",
    "\n",
    "print(\"\\n\" + \"SUMMARIZATION RESULTS\" + \"\\n\")\n",
    "print(divString(100) + \"\\n\")\n",
    "print(\"EXTRACTIVE SUMMARY PRIMARY:\" + \"\\n\", extractive_summary_primary)\n",
    "print(\"\\n\")\n",
    "print(\"ABSTRACTIVE SUMMARY PRIMARY:\" + \"\\n\", abstractive_summary_primary)\n",
    "print(\"\\n\")\n",
    "print(\"ABSTRACTIVE SUMMARY SECONDARY:\" + \"\\n\", abstractive_summary_secondary)\n",
    "print(divString(100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# ITERATIVE PIPELINE #\n",
    "######################\n",
    "\n",
    "# Initial Document Selection\n",
    "search_results_df = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "corpus = search_results_df.first_paragraph.to_list()  # Converting to a list to vectorize the entries\n",
    "initial_document = select_random_document(search_results_df).first_paragraph.values[0] # Selected a random row from the search results dataframe and extracted the first paragraph text to serve as our document\n",
    "\n",
    "# Pipeline Iteration\n",
    "iterate_pipeline(initial_document, corpus, num_iter = 5, aggregate_doc_variant = \"abstractive_secondary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_df = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "search_results = search_results_df.first_paragraph.to_list()  # Converting to a list to vectorize the entries\n",
    "corpus = vectorizer.fit_transform(search_results) # Vectorize and fit transform\n",
    "document = select_random_document(x).first_paragraph.values[0] # Selected a random row from the search results dataframe and extracted the first paragraph text to serve as our document\n",
    "# compute_cosine_similarities(document, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_random_document(x).first_paragraph.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = search_and_subset_data(covid_test_data, \"death\") # a df containing entries that have death in the first paragraph\n",
    "y = x.first_paragraph.to_list() # Converting to a list to vectorize the entries\n",
    "z = vectorizer.fit_transform(y) # Vectorize and fit transform\n",
    "\n",
    "cosine_similarities = linear_kernel(z[0], z).flatten() #Compute the similarities between a single vectorized article and the rest of the vectorized corpus\n",
    "cosine_similarities"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
